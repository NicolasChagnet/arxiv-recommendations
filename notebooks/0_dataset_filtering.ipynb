{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys; sys.path.insert(0, '..')\n",
    "from src import config,  preprocessing, arxiv_utils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the various parts of the dataset:\n",
    "- `raw_data.csv` contains selected papers from my Zotero archive\n",
    "- `papers_cited.csv` contains the list of papers I have cited when writing my own articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "all_df = pd.read_csv(config.path_data_raw / \"zotero_raw_data.csv\")\n",
    "cited_df = pd.read_csv(config.path_data_raw / \"papers_cited.csv\")\n",
    "\n",
    "# Filter data to only journal articles and keep relevant information\n",
    "all_only_articles_df = all_df.loc[all_df[\"Item Type\"] == \"journalArticle\"][\n",
    "    [\"Author\", \"Title\", \"Publication Year\", \"Extra\"]\n",
    "]\n",
    "# Extract arXiv id from the Extra section\n",
    "all_only_articles_df[\"arxiv_id\"] = all_only_articles_df.apply(\n",
    "    lambda x: arxiv_utils.extract_arxiv_identifier(x[\"Extra\"]), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match every article from the zotero library to arXiv and returns a standardized output. At the rate of 3s per request, this is very slow and should only be run once per raw data sample. Output is saved to the `interim` data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nstart = 0\n",
    "Nend = len(all_only_articles_df) - 1\n",
    "article_per_pass = 15\n",
    "Narticles = Nend - Nstart + 1\n",
    "Npasses = Narticles // article_per_pass\n",
    "library_arxiv_df = None\n",
    "# UNCOMMENT TO RUN FETCHING\n",
    "for i in range(Npasses + 1):\n",
    "    nmin = i * article_per_pass\n",
    "    nmax = min((i + 1) * article_per_pass, Nend)\n",
    "    df_tmp = arxiv_utils.get_all_article_by_id_or_title(all_only_articles_df.iloc[nmin:nmax])\n",
    "    library_arxiv_df = df_tmp if library_arxiv_df is None else pd.concat([library_arxiv_df, df_tmp])\n",
    "    library_arxiv_df.to_csv(config.path_data_interim / \"zotero_arxiv_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the arXiv entries matching the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_arxiv_df = pd.read_csv(config.path_data_interim / \"zotero_arxiv_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the citation status and interest status, arXiv id\n",
    "cited_titles = list(cited_df[\"Title\"])\n",
    "library_arxiv_df[\"is_cited_by_my_papers\"] = library_arxiv_df[\"title\"].isin(cited_titles)\n",
    "library_arxiv_df[\"is_in_library\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 667 entries, 0 to 666\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   id                     667 non-null    object\n",
      " 1   title                  667 non-null    object\n",
      " 2   authors                667 non-null    object\n",
      " 3   primary_category       667 non-null    object\n",
      " 4   categories             667 non-null    object\n",
      " 5   summary                667 non-null    object\n",
      " 6   published              667 non-null    object\n",
      " 7   doi                    667 non-null    object\n",
      " 8   is_cited_by_my_papers  667 non-null    bool  \n",
      " 9   is_in_library          667 non-null    bool  \n",
      "dtypes: bool(2), object(8)\n",
      "memory usage: 43.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for missing values in the fields of interest\n",
    "display(library_arxiv_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have so far extracted from my Zotero library various papers I was interested in (papers I cited, papers on TODO lists, etc...). Now I want to get a random sample of papers to pad the dataset and get a less skewed distribution. While not a perfect method, I take a random sample of arxiv papers (sample size is the size of the selected papers in the previous section), picked in my usual categories (hep-th, gr-qc, cond-mat.str-el) with random years (each query selects 25 papers), while checking they were not in my collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time to run: 10.0 min.\n"
     ]
    }
   ],
   "source": [
    "Npapers = 5000\n",
    "n_per_sample = 25\n",
    "n_requests = Npapers // n_per_sample\n",
    "print(f\"Estimated time to run: {3 * n_requests / 60} min.\")\n",
    "random_samples = [arxiv_utils.get_random_articles(n_articles=n_per_sample) for i in range(n_requests)]\n",
    "random_samples_dfs = [arxiv_utils.build_arxiv_df(entries) for entries in random_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the full dataset containing the papers from my library and the random samples. We first concatenate all entries and remove duplicates by arXiv id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amont of random papers already in the library to drop 4.\n",
      "Amont of duplicates dropped 17.\n"
     ]
    }
   ],
   "source": [
    "random_arxiv_df = pd.concat(random_samples_dfs, ignore_index=True)\n",
    "random_arxiv_df[\"is_cited_by_my_papers\"] = False\n",
    "random_arxiv_df[\"is_in_library\"] = False\n",
    "\n",
    "# Find duplicate id's\n",
    "dup_ids = list(set(library_arxiv_df[\"id\"]).intersection(set(random_arxiv_df[\"id\"])))\n",
    "print(f\"Amont of random papers already in the library to drop {len(dup_ids)}.\")\n",
    "\n",
    "full_df = pd.concat([library_arxiv_df, random_arxiv_df.loc[-random_arxiv_df[\"id\"].isin(dup_ids)]], ignore_index=True)\n",
    "full_df_unique = full_df.drop_duplicates(subset=\"id\", keep=\"first\")\n",
    "print(f\"Amont of duplicates dropped {len(full_df) - len(full_df_unique)}.\")\n",
    "\n",
    "# Save to file\n",
    "full_df_unique.to_csv(config.path_data_merged, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zoteroML_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
